---
title: "Text Mining _C_ (v1)"
output:
  html_document:
    keep_md: true
---

This markdown document contains the outputs I refer to in my [Substack post](https://nycdna.substack.com/p/text-mining-tom-mccarthys-c) on text mining Tom McCarthy's brilliant novel [_C_](https://www.penguinrandomhouse.com/books/201638/c-by-tom-mccarthy/), as well as the code I use to get those outputs.

A significant amount of the code below is adapted from Julia Silge and David Robinson’s [_Text Mining with R: A Tidy Approach_](https://www.tidytextmining.com/). Additionally, I imagine there are better or more elegant ways of doing many of the things I do below. My general approach was to just cobble together whatever I needed to make this project work.

### Preprocessing:
```{r message = FALSE, warning = FALSE}
library(tidyverse)

# load text file and make it into a dataframe; some minor cleanup

# I toss out the headers for the novel's parts but preserve their titles (e.g., "Caul")

c_raw <- read.delim("C.txt", header = FALSE, sep = "\n")
c_raw <- c_raw[["V1"]]
c_df <- tibble(paragraph = 1:length(c_raw), text = c_raw) |>
  mutate(chapter = cumsum(str_detect(text, regex("CHAPTER",
                                                 ignore_case = FALSE))),
         .before = paragraph) |>
  mutate(section = cumsum(str_detect(text, regex("SECTION",
                                                 ignore_case = FALSE))),
         .after = chapter) |>
  mutate(text = trimws(gsub(regex("PART [0-9]+"), "", text)))

c_df <- c_df[!grepl("CHAPTER [0-9]+|SECTION [0-9]+", c_df$text), ]

# load tidytext's stopwords list and customize it

library(tidytext)
data(stop_words)

stop_words <- bind_rows(stop_words, tibble(word = c("beneath", "set",
                                                    "sets", "setting",
                                                    "tell", "tells",
                                                    "telling", "told"),
                                           lexicon = "custom")) |>
  filter(!word %in% c("c", "c's"))

# tokenize and stem words

library(SnowballC)

c_tidy_blocked <- c_df |>
  unnest_tokens(word, text)

# add a marker every 100 words (before removing stop words) such that text can be partitioned into blocks of uniform length

block_length <- 100

c_tidy_blocked$block <- rep(seq(1, 1 + (nrow(c_tidy_blocked)) %/% block_length),
                            each = block_length,
                            length.out = nrow(c_tidy_blocked))
  
c_tidy <- c_tidy_blocked |>
  relocate(block, .after = section) |>
  anti_join(stop_words, by = "word") |>
  mutate(word = wordStem(word, language = "english"))
```

### Basic term frequency analysis:

Here are the ten most frequently appearing word stems in _C_:

```{r echo = FALSE}
c_tidy_sorted <- count(c_tidy, word, sort = TRUE)

c_tidy_top <- slice_max(c_tidy_sorted, n, n = 10, with_ties = FALSE)

c_tidy_top
```

And here are the top ten word stems in _C_ by tf–idf, after assembling a corpus of public domain novels in which to calculate tf–idf:

```{r}
library(gutenbergr)

# gather public domain novels to create a corpus in which to calculate tf–idf

novels_df <- gutenberg_download(c(768, 766, 110, 42671,
                                  526, 4300, 432, 67138),
                                meta_fields = "title",
                                mirror = "http://mirror.csclub.uwaterloo.ca/gutenberg/")

novels_df <- novels_df[!(novels_df$text == ""), ]

# tokenize and stem words in public domain novels

# I'm not worrying about extraneous text in the public domain novels (e.g., titles, tables of contents); retaining that text doesn't strike me as ultra-consequential for the limited purposes of this tf–idf analysis

novels_tidy <- novels_df |>
  unnest_tokens(word, text) |>
  mutate(word = str_extract(word, "[a-zà-ÿ0-9'’]+")) |> # get rid of underscores used for emphasis in UTF-8 encoding
  mutate(word = gsub("’", "'", word)) |>
  anti_join(stop_words, by = "word") |>
  mutate(word = wordStem(word, language = "english")) # stem all words for consistency with the approach I took with C

# sort words in each public domain novel by frequency within that novel

novels_tidy_sorted <- count(novels_tidy, title, word, sort = TRUE)

# merge dataframes

big_dataframe <- rbind(novels_tidy_sorted, c_tidy_sorted |>
                         mutate(title = "C", .before = "word"))

# calculate tf-idf

c_top_tf_idf <- big_dataframe |>
  bind_tf_idf(word, title, n) |>
  filter(title == "C") |>
  slice_max(tf_idf, n = 10, with_ties = FALSE) |>
  select(word, tf_idf)

c_top_tf_idf
```

### Bigram frequency analysis:

Here I tokenize _C_ into bigrams instead of unigrams:

```{r}
# bigrams will not be sensitive to sentence or paragraph boundaries and will include titles of novel's parts

# any bigram that contains a stop word will be removed

c_df_collapsed <- tibble(line = 1, text = paste(c_df$text, collapse = " ")) |>
  mutate(text = gsub("  ", " ", text))

c_bigrams_tidy <- c_df_collapsed |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  mutate(word1 = wordStem(word1, language = "english")) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(word2 = wordStem(word2, language = "english")) |>
  unite(bigram, word1, word2, sep = " ")
```

And here are the 20 most frequent bigrams in _C_:

```{r echo = FALSE}
c_bigrams_tidy_sorted <- count(c_bigrams_tidy, bigram, sort = TRUE)

c_bigrams_tidy_top <- slice_max(c_bigrams_tidy_sorted, n, n =20, with_ties = FALSE)

c_bigrams_tidy_top
```

Now I check to see if the top 20 bigrams in _C_ remain basically the same if we calculate them based on tf-idf instead of simple frequency:

```{r}
# get bigrams of public domain novels

novels_df_collapsed <- setNames(aggregate
                                (novels_df$text,
                                  list(novels_df$title),
                                  paste, collapse=" "),
                                c("title", "text")) |>
  mutate(text = gsub("  ", " ", text))

novels_bigrams_tidy <- novels_df_collapsed |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  mutate(word1 = str_extract(word1, "[a-zà-ÿ0-9'’]+")) |>
  mutate(word1 = gsub("’", "'", word1)) |>
  filter(!word1 %in% stop_words$word) |>
  mutate(word1 = wordStem(word1, language = "english")) |>
  mutate(word2 = str_extract(word2, "[a-zà-ÿ0-9'’]+")) |>
  mutate(word1 = gsub("’", "'", word1)) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(word2 = wordStem(word2, language = "english")) |>
  unite(bigram, word1, word2, sep = " ")

novels_bigrams_tidy_sorted <- count(novels_bigrams_tidy,
                                    title, bigram, sort = TRUE)
  
big_bigram_dataframe <- rbind(novels_bigrams_tidy_sorted,
                              c_bigrams_tidy_sorted |>
                         mutate(title = "C", .before = "bigram"))

# calculate top 20 bigrams in C by tf-idf

c_top_bigram_tf_idf <- big_bigram_dataframe |>
  bind_tf_idf(bigram, title, n) |>
  filter(title == "C") |>
  slice_max(tf_idf, n = 20, with_ties = FALSE) |>
  select(bigram, tf_idf)

c_top_bigram_tf_idf
```

They're close to being the same, with the notable omission of "c c," which disappears due to strings like "reverend Nicholas Dudley C. C." in _Ulysses_ and "'It would be an incalculable loss if,' &c., &c." in _Heart of Darkness_.

Closing out the bigrams analysis, here's a graph of the 20 most frequent bigrams in _C_:

```{r echo = FALSE, out.width = "85%", fig.align = "center", dpi = 300}
# take the bigram visualization approach described in Chapter 4 of Text Mining with R: A Tidy Approach and tweak it a bit

library(ggraph)

set_graph_style(family = "mono", background = "white")

c_bigrams_graph <- c_bigrams_tidy_top |>
  separate(bigram, c("word1", "word2"), sep = " ")

set.seed(4)

ggraph(c_bigrams_graph, layout = "fr") +
  geom_edge_link(colour = "#0060A8", alpha = 0.8) +
  geom_node_point(colour = "#ED1C24", alpha = 0.85) +
  geom_node_label(aes(label = name), size = 3, fill = "white", label.size = 0.2, repel = TRUE) +
  ggtitle("20 most frequent bigrams in C") +
  theme(title = element_text(family = "mono", face = "bold"))
```

### Frequency analysis of select words starting with "C":

Here I look at the frequency in _C_ of the following words (which I'll call "Special Words"), based on appearances per chapter, section, or block of 100 words:

```{r, echo = FALSE, comment = NA}
table1 <- matrix(c("C", "call", "carbon", "centre", "Christ",
                  "cipher", "cocaine", "code", "communication/Comintern", "connect",
                  "copper", "crypt/cryptography/etc.", "cyan/cyanide", "cyst", "sea"),
                5, 3)

prmatrix(table1, rowlab = rep("", 5), collab = rep("", 3))
```

First I look at the aggregate frequency of all Special Words across chapters, sections, and blocks of 100 words.

```{r out.width = "100%", fig.align = "center", dpi = 300}
# filter unigrams of C down to only Special Words and merge tokens I count as being instances of the same word for the limited purposes of this analysis (e.g., "carbon" and "cc")

# really not loving the inelegance of this whole chunk of code, but this works for now

special_words_filtered <- c_tidy |>
  filter(chapter > 0) |>
  filter(grepl("\\bc\\b|\\bcc\\b|\\bcall\\b|^carbon|^centr|^christ|^cipher|^cocain|\\bcode\\b|^communic|^comintern|^connect|^copper|^crypt|^cyan|^cyst|\\bsea\\b", word)) |>
  filter(!word %in% c("c'est"))

special_words_filtered_merged <- special_words_filtered |>
  mutate(word = gsub(regex("\\bcc\\b|\\bcc'd\\b|^carbon[a-zà-ÿ]+\\b"), "carbon", word)) |>
  mutate(word = gsub(regex("^centr|centr[a-zà-ÿ]+\\b"), "centre", word)) |>
  mutate(word = gsub(regex("^christ[a-zà-ÿ]+\\b"), "christ", word)) |>
  mutate(word = gsub(regex("^cipher[a-zà-ÿ]+\\b"), "cipher", word)) |>
  mutate(word = gsub(regex("^cocain"), "cocaine", word)) |>
  mutate(word = gsub(regex("^communic\\b|comintern"), "communication", word)) |>
  mutate(word = gsub(regex("^connect[a-zà-ÿ]+\\b"), "connect", word)) |>
  mutate(word = gsub(regex("^crypt[a-zà-ÿ]+\\b"), "crypt", word)) |>
  mutate(word = gsub(regex("^cyan[a-zà-ÿ]+\\b"), "cyan", word)) |>
  mutate(word = gsub(regex("^cyst[a-zà-ÿ]+\\b"), "cyst", word))

# get total number of words in each chapter/section/block (inclusive of stopwords)

total_words_by_chapter <- c_df |>
  unnest_tokens(word, text) |>
  filter(chapter > 0) |>
  count(chapter) |>
  rename(total = n)

total_words_by_section <- c_df |>
  unnest_tokens(word, text) |>
  filter(section > 0) |>
  count(section) |>
  rename(total = n)

total_words_by_block <- c_tidy_blocked |>
  count(block) |>
  rename(total = n)

# get number of Special Words by chapter/section/block

special_words_sorted_by_chapter <- count(special_words_filtered_merged, chapter, word, sort = TRUE) |>
  arrange(word)

special_words_sorted_by_section <- count(special_words_filtered_merged, section, word, sort = TRUE) |>
  arrange(word)

special_words_sorted_by_block <- count(special_words_filtered_merged, block, word, sort = TRUE) |>
  arrange(word)

# calculate and plot aggregate frequency of all Special Words by chapter/section/block

aggregate_word_frequency_by_chapter <- special_words_sorted_by_chapter |>
  group_by(chapter) |>
  summarise(n = sum(n)) |>
  left_join(total_words_by_chapter, by = "chapter") |>
  mutate(percentage = n/total)

aggregate_word_frequency_by_section <- special_words_sorted_by_section |>
  group_by(section) |>
  summarise(n = sum(n)) |>
  left_join(total_words_by_section, by = "section") |>
  mutate(percentage = n/total)

aggregate_word_frequency_by_block <- special_words_sorted_by_block |>
  group_by(block) |>
  summarise(n = sum(n)) |>
  left_join(total_words_by_block, by = "block") |>
  mutate(percentage = n/total)

library(patchwork)
theme_set(theme_classic())

plot1.1 <- ggplot(aggregate_word_frequency_by_chapter, aes(chapter, percentage)) +
  geom_line(colour = "#ED1C24") +
  scale_x_continuous(limits = c(1,12), breaks = seq(0, 100), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme(axis.title.y = element_text(vjust = 2, family = "mono"),
        axis.title.x = element_text(family = "mono"))

plot1.2 <- ggplot(aggregate_word_frequency_by_section, aes(section, percentage)) +
  geom_line(colour = "#ED1C24") +
  scale_x_continuous(limits = c(1,39), breaks = seq(0, 100, by = 5), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(family = "mono"))

plot1.3 <- ggplot(aggregate_word_frequency_by_block, aes(block, percentage)) +
  geom_line(colour = "#ED1C24") +
  scale_x_continuous(limits = c(1,1194), breaks = seq(0, 1194, by = 200), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(family = "mono"))

plot1.1 + plot1.2 + plot1.3 + plot_annotation(title = "aggregate frequency of all Special Words in C", subtitle = "by chapter, section, and block of 100 words") & theme(plot.title = element_text(family = "mono", face = "bold"), plot.subtitle = element_text(family = "mono"))
```

I calculate Pearson correlation coefficients for the above data to check for any linear trend:

```{r echo = FALSE, comment = NA}
cor1 <- cor.test(~ chapter + percentage, aggregate_word_frequency_by_chapter)

cor2 <- cor.test(~ section + percentage, aggregate_word_frequency_by_section)

cor3 <- cor.test(~ block + percentage, aggregate_word_frequency_by_block)

cat(" chapter\n", "correlation:", cor1$estimate, "\n", "p-value:", cor1$p.value, "\n\n",
    "section\n", "correlation:", cor2$estimate, "\n", "p-value:", cor2$p.value, "\n\n",
    "block\n", "correlation:", cor3$estimate, "\n", "p-value:", cor3$p.value)
```

There's a positive correlation between frequency of special words and the forward temporal progression of the novel. If calculated based on 100-word blocks, the increase has a modest level of statistical significance.

Now I look at the frequency of individual Special Words across chapters/sections/blocks:

```{r out.width = "100%", fig.align = "center", dpi = 300}
# calculate frequency of individual Special Words across chapters/sections/blocks:

special_word_frequency_by_chapter <- special_words_sorted_by_chapter |>
  pivot_wider(names_from = word, values_from = n, values_fill = 0) |>
  left_join(total_words_by_chapter, by = "chapter") |>
  mutate(across(c(2:16), ~ .x/total))

special_word_frequency_by_section <- special_words_sorted_by_section |>
  pivot_wider(names_from = word, values_from = n, values_fill = 0) |>
  left_join(total_words_by_section, by = "section") |>
  mutate(across(c(2:16), ~ .x/total))

special_word_frequency_by_block <- special_words_sorted_by_block |>
  pivot_wider(names_from = word, values_from = n, values_fill = 0) |>
  left_join(total_words_by_block, by = "block") |>
  mutate(across(c(2:16), ~ .x/total))

# plot individual frequencies of all 15 Special Words words across chapters/sections/blocks:

plot2.1 <- ggplot(special_word_frequency_by_chapter, aes(chapter)) +
  geom_line(aes(y = c, colour = "c")) +
  geom_line(aes(y = call, colour = "call")) +
  geom_line(aes(y = carbon, colour = "carbon")) +
  geom_line(aes(y = centre, colour = "centre")) +
  geom_line(aes(y = christ, colour = "christ")) +
  geom_line(aes(y = cipher, colour = "cipher")) +
  geom_line(aes(y = cocaine, colour = "cocaine")) +
  geom_line(aes(y = code, colour = "code")) +
  geom_line(aes(y = communication, colour = "communication")) +
  geom_line(aes(y = connect, colour = "connect")) +
  geom_line(aes(y = copper, colour = "copper")) +
  geom_line(aes(y = crypt, colour = "crypt")) +
  geom_line(aes(y = cyan, colour = "cyan")) +
  geom_line(aes(y = cyst, colour = "cyst")) +
  geom_line(aes(y = sea, colour = "sea")) +
  scale_x_continuous(limits = c(1, 12), breaks = seq(0, 100), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_colour_manual(values = c("#ED1C24","#EF7B10","#0060A8", "#B26300", "#FFD329", "#007D32", "#F4A9BE", "#A1A5A7", "#9B0058", "#0019A8", "#00FFA1", "#9364CC", "cyan", "#6CBE45", "#BD3844")) +
  labs(x = "chapter", y = "frequency", colour = "word") +
  ggtitle("frequency of Special Words in C", subtitle = "by chapter") +
  theme(text = element_text(family = "mono"), plot.title = element_text(face = "bold"))

plot2.2 <- ggplot(special_word_frequency_by_section, aes(section)) +
  geom_line(aes(y = c, colour = "c")) +
  geom_line(aes(y = call, colour = "call")) +
  geom_line(aes(y = carbon, colour = "carbon")) +
  geom_line(aes(y = centre, colour = "centre")) +
  geom_line(aes(y = christ, colour = "christ")) +
  geom_line(aes(y = cipher, colour = "cipher")) +
  geom_line(aes(y = cocaine, colour = "cocaine")) +
  geom_line(aes(y = code, colour = "code")) +
  geom_line(aes(y = communication, colour = "communication")) +
  geom_line(aes(y = connect, colour = "connect")) +
  geom_line(aes(y = copper, colour = "copper")) +
  geom_line(aes(y = crypt, colour = "crypt")) +
  geom_line(aes(y = cyan, colour = "cyan")) +
  geom_line(aes(y = cyst, colour = "cyst")) +
  geom_line(aes(y = sea, colour = "sea")) +
  scale_x_continuous(limits = c(1, 39), breaks = seq(0, 100, by = 5), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_colour_manual(values = c("#ED1C24","#EF7B10","#0060A8", "#B26300", "#FFD329", "#007D32", "#F4A9BE", "#A1A5A7", "#9B0058", "#0019A8", "#00FFA1", "#9364CC", "cyan", "#6CBE45", "#BD3844")) +
  labs(x = "section", y = "frequency", colour = "word") +
  ggtitle("frequency of Special Words in C", subtitle = "by section") +
  theme(text = element_text(family = "mono"), plot.title = element_text(face = "bold"))

plot2.3 <- ggplot(special_word_frequency_by_block, aes(block)) +
  geom_line(aes(y = c, colour = "c")) +
  geom_line(aes(y = call, colour = "call")) +
  geom_line(aes(y = carbon, colour = "carbon")) +
  geom_line(aes(y = centre, colour = "centre")) +
  geom_line(aes(y = christ, colour = "christ")) +
  geom_line(aes(y = cipher, colour = "cipher")) +
  geom_line(aes(y = cocaine, colour = "cocaine")) +
  geom_line(aes(y = code, colour = "code")) +
  geom_line(aes(y = communication, colour = "communication")) +
  geom_line(aes(y = connect, colour = "connect")) +
  geom_line(aes(y = copper, colour = "copper")) +
  geom_line(aes(y = crypt, colour = "crypt")) +
  geom_line(aes(y = cyan, colour = "cyan")) +
  geom_line(aes(y = cyst, colour = "cyst")) +
  geom_line(aes(y = sea, colour = "sea")) +
  scale_x_continuous(limits = c(1, 1194), breaks = seq(0, 1194, by = 100), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_colour_manual(values = c("#ED1C24","#EF7B10","#0060A8", "#B26300", "#FFD329", "#007D32", "#F4A9BE", "#A1A5A7", "#9B0058", "#0019A8", "#00FFA1", "#9364CC", "cyan", "#6CBE45", "#BD3844")) +
  labs(x = "block", y = "frequency", colour = "word") +
  ggtitle("frequency of Special Words in C", subtitle = "by block of 100 words") +
  theme(text = element_text(family = "mono"), plot.title = element_text(face = "bold"))

plot2.1
```

Here too, I calculate Pearson correlation coefficients for the above data to check for linear trends:

```{r echo = FALSE, comment = NA}
# run Pearson correlation tests between each Special Word, on the one hand, and chapters/sections/blocks in C, on the other:

cor_tests_by_chapter <- special_word_frequency_by_chapter |>
  select(c(2:16)) |>
  apply(2, cor.test, special_word_frequency_by_chapter$chapter)

cor_tests_by_section <- special_word_frequency_by_section |>
  select(c(2:16)) |>
  apply(2, cor.test, special_word_frequency_by_section$section)

cor_tests_by_block <- special_word_frequency_by_block |>
  select(c(2:16)) |>
  apply(2, cor.test, special_word_frequency_by_block$block)

# there has to be a better way of getting all the correlation values and p-values to print in a table:

table2 <- matrix(c(cor_tests_by_chapter$c$estimate,
                   cor_tests_by_chapter$c$p.value,
                   cor_tests_by_section$c$estimate,
                   cor_tests_by_section$c$p.value,
                   cor_tests_by_block$c$estimate,
                   cor_tests_by_block$c$p.value,
                   cor_tests_by_chapter$call$estimate,
                   cor_tests_by_chapter$call$p.value,
                   cor_tests_by_section$call$estimate,
                   cor_tests_by_section$call$p.value,
                   cor_tests_by_block$call$estimate,
                   cor_tests_by_block$call$p.value,
                   cor_tests_by_chapter$carbon$estimate,
                   cor_tests_by_chapter$carbon$p.value,
                   cor_tests_by_section$carbon$estimate,
                   cor_tests_by_section$carbon$p.value,
                   cor_tests_by_block$carbon$estimate,
                   cor_tests_by_block$carbon$p.value,
                   cor_tests_by_chapter$centre$estimate,
                   cor_tests_by_chapter$centre$p.value,
                   cor_tests_by_section$centre$estimate,
                   cor_tests_by_section$centre$p.value,
                   cor_tests_by_block$centre$estimate,
                   cor_tests_by_block$centre$p.value,
                   cor_tests_by_chapter$christ$estimate,
                   cor_tests_by_chapter$christ$p.value,
                   cor_tests_by_section$christ$estimate,
                   cor_tests_by_section$christ$p.value,
                   cor_tests_by_block$christ$estimate,
                   cor_tests_by_block$christ$p.value,
                   cor_tests_by_chapter$cipher$estimate,
                   cor_tests_by_chapter$cipher$p.value,
                   cor_tests_by_section$cipher$estimate,
                   cor_tests_by_section$cipher$p.value,
                   cor_tests_by_block$cipher$estimate,
                   cor_tests_by_block$cipher$p.value,
                   cor_tests_by_chapter$cocaine$estimate,
                   cor_tests_by_chapter$cocaine$p.value,
                   cor_tests_by_section$cocaine$estimate,
                   cor_tests_by_section$cocaine$p.value,
                   cor_tests_by_block$cocaine$estimate,
                   cor_tests_by_block$cocaine$p.value,
                   cor_tests_by_chapter$code$estimate,
                   cor_tests_by_chapter$code$p.value,
                   cor_tests_by_section$code$estimate,
                   cor_tests_by_section$code$p.value,
                   cor_tests_by_block$code$estimate,
                   cor_tests_by_block$code$p.value,
                   cor_tests_by_chapter$communication$estimate,
                   cor_tests_by_chapter$communication$p.value,
                   cor_tests_by_section$communication$estimate,
                   cor_tests_by_section$communication$p.value,
                   cor_tests_by_block$communication$estimate,
                   cor_tests_by_block$communication$p.value,
                   cor_tests_by_chapter$connect$estimate,
                   cor_tests_by_chapter$connect$p.value,
                   cor_tests_by_section$connect$estimate,
                   cor_tests_by_section$connect$p.value,
                   cor_tests_by_block$connect$estimate,
                   cor_tests_by_block$connect$p.value,
                   cor_tests_by_chapter$copper$estimate,
                   cor_tests_by_chapter$copper$p.value,
                   cor_tests_by_section$copper$estimate,
                   cor_tests_by_section$copper$p.value,
                   cor_tests_by_block$copper$estimate,
                   cor_tests_by_block$copper$p.value,
                   cor_tests_by_chapter$crypt$estimate,
                   cor_tests_by_chapter$crypt$p.value,
                   cor_tests_by_section$crypt$estimate,
                   cor_tests_by_section$crypt$p.value,
                   cor_tests_by_block$crypt$estimate,
                   cor_tests_by_block$crypt$p.value,
                   cor_tests_by_chapter$cyan$estimate,
                   cor_tests_by_chapter$cyan$p.value,
                   cor_tests_by_section$cyan$estimate,
                   cor_tests_by_section$cyan$p.value,
                   cor_tests_by_block$cyan$estimate,
                   cor_tests_by_block$cyan$p.value,
                   cor_tests_by_chapter$cyst$estimate,
                   cor_tests_by_chapter$cyst$p.value,
                   cor_tests_by_section$cyst$estimate,
                   cor_tests_by_section$cyst$p.value,
                   cor_tests_by_block$cyst$estimate,
                   cor_tests_by_block$cyst$p.value,
                   cor_tests_by_chapter$sea$estimate,
                   cor_tests_by_chapter$sea$p.value,
                   cor_tests_by_section$sea$estimate,
                   cor_tests_by_section$sea$p.value,
                   cor_tests_by_block$sea$estimate,
                   cor_tests_by_block$sea$p.value),
                15, 6, byrow = TRUE)

prmatrix(table2,
         rowlab = c("C", "call", "carbon", "centre", "Christ",
                  "cipher", "cocaine", "code", "communication/Comintern", "connect",
                  "copper", "crypt/cryptography/etc.", "cyan/cyanide", "cyst", "sea"),
         collab = c("chapter (cor)", "(p-value)",
                    "section (cor)", "(p-value)",
                    "block (cor)", "(p-value)"))
```

### Pairwise correlation analysis of special words:

Finally, I look at the phi coefficients for pairs of Special Words within the same chapter/section/block of _C_ (that is, how correlated the appearance of one Special Word within a given chapter/section/block is with the appearance of another Special Word within that same chapter/section/block).

```{r}
library(widyr)

special_word_pairs <- special_words_filtered_merged |>
  group_by(word)
  
word_pairs_by_chapter <- pairwise_cor(special_word_pairs, word, chapter, sort = TRUE)

word_pairs_by_section <- pairwise_cor(special_word_pairs, word, section, sort = TRUE)

word_pairs_by_block <- pairwise_cor(special_word_pairs, word, block, sort = TRUE)
```

Here are the 10 Special Words most correlated with "c" by chapter:

```{r}
word_pairs_by_chapter |>
  filter(item2 == "c") |>
  slice_max(correlation, n = 10, with_ties = FALSE)
```

Here are the 10 Special Words most correlated with "call" by chapter:

```{r}
word_pairs_by_chapter |>
  filter(item2 == "call") |>
  slice_max(correlation, n = 10, with_ties = FALSE)
```

And here's a visualization of every pair of Special Words that has a positive correlation within chapters (left) and 100-word blocks (right) of _C_:

```{r echo = FALSE, out.width = "100%", fig.align = "center", dpi = 300}
# take the pairwise correlation visualization approach described in Chapter 4 of Text Mining with R: A Tidy Approach and tweak it a bit

set.seed(44)

plot3.1 <- word_pairs_by_chapter |>
  filter(correlation > 0) |>
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), colour = "#0060A8", show.legend = FALSE) +
  geom_node_point(colour = "#ED1C24", size = 2.5, alpha = 0.85) +
  geom_node_label(aes(label = name), size = 3, fill = "white", label.size = 0.2, repel = TRUE) +
  ggtitle("chapter") +
  theme(title = element_text(family = "mono", face = "bold"))

plot3.2 <- word_pairs_by_section |>
  filter(correlation > 0) |>
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), colour = "#0060A8", show.legend = FALSE) +
  geom_node_point(colour = "#ED1C24", size = 2.5, alpha = 0.85) +
  geom_node_label(aes(label = name), size = 3, fill = "white", label.size = 0.2, repel = TRUE) +
  ggtitle("section") +
  theme(title = element_text(family = "mono", face = "bold"))

plot3.3 <- word_pairs_by_block |>
  filter(correlation > 0) |>
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), colour = "#0060A8", show.legend = FALSE) +
  geom_node_point(colour = "#ED1C24", size = 2.5, alpha = 0.85) +
  geom_node_label(aes(label = name), size = 3, fill = "white", label.size = 0.2, repel = TRUE) +
  ggtitle("block") +
  theme(title = element_text(family = "mono", face = "bold"))

plot3.1 + plot3.3
```

### Coda

Here I just generate a couple of added visualizations I use in my Substack post:

```{r echo = FALSE, out.width = "100%", fig.align = "center", dpi = 300}

plot1.3 + plot_annotation(title = "aggregate frequency of all Special Words in C", subtitle = "by block of 100 words") & theme(plot.title = element_text(family = "mono", face = "bold"), plot.subtitle = element_text(family = "mono"))

plot3.1 + ggtitle("")
```