---
title: "Text Mining _C_ (v1)"
output:
  html_document:
    keep_md: true
---

This markdown document contains the outputs I refer to in my Substack post on text mining Tom McCarthy's brilliant novel _C_, as well as the code I use to get those outputs. Text file of _C_ not included, _bien sûr_.

A significant amount of the code below is adapted from Julia Silge and David Robinson’s [_Text Mining with R: A Tidy Approach_](https://www.tidytextmining.com/). Additionally, I imagine there are better or more elegant ways of doing many of the things I do below. My general approach was to just cobble together whatever I needed to make this project work.

### Preprocessing:
```{r message=FALSE, warning=FALSE}
library(tidyverse)

# loading text file and making it into a dataframe; some minor cleanup

# note that I toss out part headers but preserve titles of novel's parts (e.g., "Caul") as formatted in my .txt file

c_raw <- read.delim("C.txt", header = FALSE, sep = "\n")
c_raw <- c_raw[["V1"]]
c_df <- tibble(paragraph = 1:length(c_raw), text = c_raw) |>
  mutate(chapter = cumsum(str_detect(text, regex("CHAPTER",
                                                 ignore_case = FALSE))),
         .before = paragraph) |>
  mutate(section = cumsum(str_detect(text, regex("SECTION",
                                                 ignore_case = FALSE))),
         .after = chapter) |>
  mutate(text = trimws(gsub(regex("PART [0-9]+"), "", text)))

c_df <- c_df[!grepl("CHAPTER [0-9]+|SECTION [0-9]+", c_df$text), ]

# loading tidytext's stopwords list and customizing it

library(tidytext)
data(stop_words)

stop_words <- bind_rows(stop_words, tibble(word = c("beneath", "set",
                                                    "sets", "setting",
                                                    "tell", "tells",
                                                    "telling", "told"),
                                           lexicon = "custom")) |>
  filter(!word %in% c("c", "c's"))

# tokenizing and stemming words

library(SnowballC)

c_tidy_blocked <- c_df |>
  unnest_tokens(word, text)

# adding a marker every 100 words (before removing stop words) such that text can be partitioned into blocks of "uniform" length

block_length <- 100

c_tidy_blocked$block <- rep(seq(1, 1 + (nrow(c_tidy_blocked)) %/% block_length),
                            each = block_length,
                            length.out = nrow(c_tidy_blocked))
  
c_tidy <- c_tidy_blocked |>
  relocate(block, .after = section) |>
  anti_join(stop_words, by = "word") |>
  mutate(word = wordStem(word, language = "english"))
```

### Basic term frequency analysis:

Here are the ten most frequently appearing word stems in _C_:

```{r}
c_tidy_sorted <- count(c_tidy, word, sort = TRUE)

c_tidy_top <- slice_max(c_tidy_sorted, n, n = 10, with_ties = FALSE)

c_tidy_top
```

And here are the top 10 word stems in _C_ by tf–idf, calculated within a corpus of novels:

```{r}
library(gutenbergr)

# gathering public domain novels to create a corpus in which to calculate tf–idf

novels_df <- gutenberg_download(c(768, 766, 110, 42671,
                                  526, 4300, 432, 67138),
                                meta_fields = "title",
                                mirror = "http://mirror.csclub.uwaterloo.ca/gutenberg/")

novels_df <- novels_df[!(novels_df$text == ""), ]

# tokenizing and stemming words in public domain novels; not worrying about extraneous text (e.g., titles, tables of contents) because retaining them doesn't strike me as ultra-consequential in this context

novels_tidy <- novels_df |>
  unnest_tokens(word, text) |>
  mutate(word = str_extract(word, "[a-zà-ÿ0-9'’]+")) |> # getting rid of underscores used for emphasis in UTF-8 encoding
  mutate(word = gsub("’", "'", word)) |>
  anti_join(stop_words, by = "word") |>
  mutate(word = wordStem(word, language = "english")) # stemming all words for consistency with the approach I took with C

# sort words in public domain novels by frequency in each novel

novels_tidy_sorted <- count(novels_tidy, title, word, sort = TRUE)

# merging dataframes

big_dataframe <- rbind(novels_tidy_sorted, c_tidy_sorted |>
                         mutate(title = "C", .before = "word"))

# calculating tf-idf

c_top_tf_idf <- big_dataframe |>
  bind_tf_idf(word, title, n) |>
  filter(title == "C") |>
  slice_max(tf_idf, n = 10, with_ties = FALSE) |>
  select(word, tf_idf)

c_top_tf_idf
```

### Bigram frequency analysis:

Here I tokenize _C_ into bigrams instead of unigrams:

```{r}
# bigrams will not be sensitive to sentence or paragraph boundaries and will include titles of novel's parts; any bigram that contains a stop word will be removed

c_df_collapsed <- tibble(line = 1, text = paste(c_df$text, collapse = " ")) |>
  mutate(text = gsub("  ", " ", text))

c_bigrams_tidy <- c_df_collapsed |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  mutate(word1 = wordStem(word1, language = "english")) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(word2 = wordStem(word2, language = "english")) |>
  unite(bigram, word1, word2, sep = " ")

# 20 most frequently appearing bigrams in C:

c_bigrams_tidy_sorted <- count(c_bigrams_tidy, bigram, sort = TRUE)

c_bigrams_tidy_top <- slice_max(c_bigrams_tidy_sorted, n, n =20, with_ties = FALSE)

c_bigrams_tidy_top
```

Here I check to see if the top bigrams in _C_ remain basically the same if we calculate based on tf-idf instead of simple bigram frequency:

```{r}
# getting bigrams of public domain novels

novels_df_collapsed <- setNames(aggregate
                                (novels_df$text,
                                  list(novels_df$title),
                                  paste, collapse=" "),
                                c("title", "text")) |>
  mutate(text = gsub("  ", " ", text))

novels_bigrams_tidy <- novels_df_collapsed |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  mutate(word1 = str_extract(word1, "[a-zà-ÿ0-9'’]+")) |>
  mutate(word1 = gsub("’", "'", word1)) |>
  filter(!word1 %in% stop_words$word) |>
  mutate(word1 = wordStem(word1, language = "english")) |>
  mutate(word2 = str_extract(word2, "[a-zà-ÿ0-9'’]+")) |>
  mutate(word1 = gsub("’", "'", word1)) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(word2 = wordStem(word2, language = "english")) |>
  unite(bigram, word1, word2, sep = " ")

novels_bigrams_tidy_sorted <- count(novels_bigrams_tidy,
                                    title, bigram, sort = TRUE)
  
big_bigram_dataframe <- rbind(novels_bigrams_tidy_sorted,
                              c_bigrams_tidy_sorted |>
                         mutate(title = "C", .before = "bigram"))

c_top_bigram_tf_idf <- big_bigram_dataframe |>
  bind_tf_idf(bigram, title, n) |>
  filter(title == "C") |>
  slice_max(tf_idf, n = 20, with_ties = FALSE) |>
  select(bigram, tf_idf)

c_top_bigram_tf_idf
```

Here's a graph of the 20 most freuqent bigrams in _C_:

```{r out.width = "85%", fig.align = "center", dpi = 300}
# taking the bigram visualization approach described in Chapter 4 of Text Mining with R: A Tidy Approach and tweaking it a bit

library(ggraph)
set_graph_style(family = "mono", background = "white")

c_bigrams_graph <- c_bigrams_tidy_top |>
  separate(bigram, c("word1", "word2"), sep = " ")

set.seed(4)

ggraph(c_bigrams_graph, layout = "fr") +
  geom_edge_link(alpha=0.3) +
  geom_node_point(colour = "#ED1C24", alpha = 8.5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1, repel = TRUE) +
  theme_void()
```

### Frequency analysis of specified words:

Here I look at the frequency in _C_ of the following words, based on appearances per chapter or section:

```{r, echo = FALSE, comment = NA}
table1 <- matrix(c("C", "call", "carbon", "centre", "Christ",
                  "cipher", "cocaine", "code", "communication/Comintern", "connect",
                  "copper", "crypt/cryptography/etc.", "cyan/cyanide", "cyst", "sea"),
                5, 3)

prmatrix(table1, rowlab = rep("", 5), collab = rep("", 3))
```

First I look at the aggregate frequency of all 20 words across chapters, sections, and blocks of 100 words.

```{r out.width = "100%", fig.align = "center", dpi = 300}
# filtering unigrams of _C_ down to only the specified words and merging tokens I count as being instances of the same word for the limited purposes of this analysis (e.g., "carbon" and "cc")

# not loving the inelegance of this whole chunk of code, and I know I can do this in a better way if I mess around with it more, but this works for now

specified_words_filtered <- c_tidy |>
  filter(chapter > 0) |>
  filter(grepl("\\bc\\b|\\bcc\\b|\\bcall\\b|^carbon|^centr|^christ|^cipher|^cocain|\\bcode\\b|^communic|^comintern|^connect|^copper|^crypt|^cyan|^cyst|\\bsea\\b", word)) |>
  filter(!word %in% c("c'est"))

specified_words_filtered_merged <- specified_words_filtered |>
  mutate(word = gsub(regex("\\bcc\\b|\\bcc'd\\b|^carbon[a-zà-ÿ]+\\b"), "carbon", word)) |>
  mutate(word = gsub(regex("^centr|centr[a-zà-ÿ]+\\b"), "centre", word)) |>
  mutate(word = gsub(regex("^christ[a-zà-ÿ]+\\b"), "christ", word)) |>
  mutate(word = gsub(regex("^cipher[a-zà-ÿ]+\\b"), "cipher", word)) |>
  mutate(word = gsub(regex("^cocain"), "cocaine", word)) |>
  mutate(word = gsub(regex("^communic\\b|comintern"), "communication", word)) |>
  mutate(word = gsub(regex("^connect[a-zà-ÿ]+\\b"), "connect", word)) |>
  mutate(word = gsub(regex("^crypt[a-zà-ÿ]+\\b"), "crypt", word)) |>
  mutate(word = gsub(regex("^cyan[a-zà-ÿ]+\\b"), "cyan", word)) |>
  mutate(word = gsub(regex("^cyst[a-zà-ÿ]+\\b"), "cyst", word))

# getting total number of words by chapter/section/block (inclusive of stopwords)

total_words_by_chapter <- c_df |>
  unnest_tokens(word, text) |>
  filter(chapter > 0) |>
  count(chapter) |>
  rename(total = n)

total_words_by_section <- c_df |>
  unnest_tokens(word, text) |>
  filter(section > 0) |>
  count(section) |>
  rename(total = n)

total_words_by_block <- c_tidy_blocked |>
  count(block) |>
  rename(total = n)

# getting number of specified words by chapter/section/block

specified_words_sorted_by_chapter <- count(specified_words_filtered_merged, chapter, word, sort = TRUE) |>
  arrange(word)

specified_words_sorted_by_section <- count(specified_words_filtered_merged, section, word, sort = TRUE) |>
  arrange(word)

specified_words_sorted_by_block <- count(specified_words_filtered_merged, block, word, sort = TRUE) |>
  arrange(word)

# calculating and plotting aggregate frequency of all 20 words by chapter/section (not particularly interesting):

aggregate_word_frequency_by_chapter <- specified_words_sorted_by_chapter |>
  group_by(chapter) |>
  summarise(n = sum(n)) |>
  left_join(total_words_by_chapter, by = "chapter") |>
  mutate(percentage = n/total)

aggregate_word_frequency_by_section <- specified_words_sorted_by_section |>
  group_by(section) |>
  summarise(n = sum(n)) |>
  left_join(total_words_by_section, by = "section") |>
  mutate(percentage = n/total)

aggregate_word_frequency_by_block <- specified_words_sorted_by_block |>
  group_by(block) |>
  summarise(n = sum(n)) |>
  left_join(total_words_by_block, by = "block") |>
  mutate(percentage = n/total)

library(patchwork)
theme_set(theme_classic())

plot1 <- ggplot(aggregate_word_frequency_by_chapter, aes(chapter, percentage)) +
  geom_line(color = "#ED1C24") +
  scale_x_continuous(limits = c(1,12), breaks = seq(0, 100), expand = c(0, 0)) +
  theme(axis.title.y = element_text(vjust = 2, family = "mono"),
        axis.title.x = element_text(family = "mono"))

plot2 <- ggplot(aggregate_word_frequency_by_section, aes(section, percentage)) +
  geom_line(color = "#ED1C24") +
  scale_x_continuous(limits = c(1,39), breaks = seq(0, 100, by = 5), expand = c(0, 0)) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(family = "mono"))

plot3 <- ggplot(aggregate_word_frequency_by_block, aes(block, percentage)) +
  geom_line(color = "#ED1C24") +
  scale_x_continuous(limits = c(1,1194), breaks = seq(0, 1194, by = 200), expand = c(0, 0)) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(family = "mono"))

plot1 + plot2 + plot3 + plot_annotation(title = "aggregate frequency of all Special Words in C", subtitle = "by chapter, section, and block of 100 words") & theme(plot.title = element_text(family = "mono", face = "bold"), plot.subtitle = element_text(family = "mono"))
```

I calculate Pearson correlation coefficients for the above data to check for any linear trend:

```{r echo = FALSE, comment = NA}
cor1 <- cor.test(~ chapter + percentage, aggregate_word_frequency_by_chapter)

cor2 <- cor.test(~ section + percentage, aggregate_word_frequency_by_section)

cor3 <- cor.test(~ block + percentage, aggregate_word_frequency_by_block)

cat(" chapter\n", "correlation:", cor1$estimate, "\n", "p-value:", cor1$p.value, "\n\n",
    "section\n", "correlation:", cor2$estimate, "\n", "p-value:", cor2$p.value, "\n\n",
    "block\n", "correlation:", cor3$estimate, "\n", "p-value:", cor3$p.value)
```
There's a positive correlation between frequency of specified words and the progression of the novel. If calculated based on 100-word blocks, the increase has a modest level of significance.

Now I look at the frequency of specific words across chapters/sections/blocks:

```{r out.width = "100%", fig.align = "center", dpi = 300}
# calculating frequency of specified words across chapters/sections/blocks:

specified_word_frequency_by_chapter <- specified_words_sorted_by_chapter |>
  pivot_wider(names_from = word, values_from = n, values_fill = 0) |>
  left_join(total_words_by_chapter, by = "chapter") |>
  mutate(across(c(2:16), ~ .x/total))

specified_word_frequency_by_section <- specified_words_sorted_by_section |>
  pivot_wider(names_from = word, values_from = n, values_fill = 0) |>
  left_join(total_words_by_section, by = "section") |>
  mutate(across(c(2:16), ~ .x/total))

specified_word_frequency_by_block <- specified_words_sorted_by_block |>
  pivot_wider(names_from = word, values_from = n, values_fill = 0) |>
  left_join(total_words_by_block, by = "block") |>
  mutate(across(c(2:16), ~ .x/total))

# plotting individual frequencies of all 20 words across chapters/sections/blocks:

plot1 <- ggplot(specified_word_frequency_by_chapter, aes(chapter)) +
  geom_line(aes(y = c, colour = "c")) +
  geom_line(aes(y = call, colour = "call")) +
  geom_line(aes(y = carbon, colour = "carbon")) +
  geom_line(aes(y = centre, colour = "centre")) +
  geom_line(aes(y = christ, colour = "christ")) +
  geom_line(aes(y = cipher, colour = "cipher")) +
  geom_line(aes(y = cocaine, colour = "cocaine")) +
  geom_line(aes(y = code, colour = "code")) +
  geom_line(aes(y = communication, colour = "communication")) +
  geom_line(aes(y = connect, colour = "connect")) +
  geom_line(aes(y = copper, colour = "copper")) +
  geom_line(aes(y = crypt, colour = "crypt")) +
  geom_line(aes(y = cyan, colour = "cyan")) +
  geom_line(aes(y = cyst, colour = "cyst")) +
  geom_line(aes(y = sea, colour = "sea")) +
  scale_x_continuous(limits = c(1, 12), breaks = seq(0, 100), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_color_manual(values = c("#ED1C24","#EF7B10","#0060A8", "#B26300", "#FFD329", "#007D32", "#F4A9BE", "#A1A5A7", "#9B0058", "#0019A8", "#00FFA1", "#9364CC", "cyan", "#6CBE45", "#BD3844")) +
  labs(x = "chapter", y = "frequency", color="word") +
  ggtitle("frequency of Special Words from C by chapter") +
  theme(axis.title.x = element_text(family = "mono"),
        axis.title.y = element_text(family = "mono"),
        plot.title = element_text(family = "mono"),
        plot.subtitle = element_text(family = "mono"),
        legend.title = element_text(family = "mono"),
        legend.text = element_text(family = "mono"))

plot2 <- ggplot(specified_word_frequency_by_section, aes(section)) +
  geom_line(aes(y = c, colour = "c")) +
  geom_line(aes(y = call, colour = "call")) +
  geom_line(aes(y = carbon, colour = "carbon")) +
  geom_line(aes(y = centre, colour = "centre")) +
  geom_line(aes(y = christ, colour = "christ")) +
  geom_line(aes(y = cipher, colour = "cipher")) +
  geom_line(aes(y = cocaine, colour = "cocaine")) +
  geom_line(aes(y = code, colour = "code")) +
  geom_line(aes(y = communication, colour = "communication")) +
  geom_line(aes(y = connect, colour = "connect")) +
  geom_line(aes(y = copper, colour = "copper")) +
  geom_line(aes(y = crypt, colour = "crypt")) +
  geom_line(aes(y = cyan, colour = "cyan")) +
  geom_line(aes(y = cyst, colour = "cyst")) +
  geom_line(aes(y = sea, colour = "sea")) +
  scale_x_continuous(limits = c(1, 39), breaks = seq(0, 100, by = 5), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_color_manual(values = c("#ED1C24","#EF7B10","#0060A8", "#B26300", "#FFD329", "#007D32", "#F4A9BE", "#A1A5A7", "#9B0058", "#0019A8", "#00FFA1", "#9364CC", "cyan", "#6CBE45", "#BD3844")) +
  labs(x = "section", y = "frequency", color="word") +
  ggtitle("frequency of Special Words from C by section") +
  theme(axis.title.x = element_text(family = "mono"),
        axis.title.y = element_text(family = "mono"),
        plot.title = element_text(family = "mono"),
        plot.subtitle = element_text(family = "mono"),
        legend.title = element_text(family = "mono"),
        legend.text = element_text(family = "mono"))

plot3 <- ggplot(specified_word_frequency_by_block, aes(block)) +
  geom_line(aes(y = c, colour = "c")) +
  geom_line(aes(y = call, colour = "call")) +
  geom_line(aes(y = carbon, colour = "carbon")) +
  geom_line(aes(y = centre, colour = "centre")) +
  geom_line(aes(y = christ, colour = "christ")) +
  geom_line(aes(y = cipher, colour = "cipher")) +
  geom_line(aes(y = cocaine, colour = "cocaine")) +
  geom_line(aes(y = code, colour = "code")) +
  geom_line(aes(y = communication, colour = "communication")) +
  geom_line(aes(y = connect, colour = "connect")) +
  geom_line(aes(y = copper, colour = "copper")) +
  geom_line(aes(y = crypt, colour = "crypt")) +
  geom_line(aes(y = cyan, colour = "cyan")) +
  geom_line(aes(y = cyst, colour = "cyst")) +
  geom_line(aes(y = sea, colour = "sea")) +
  scale_x_continuous(limits = c(1, 1194), breaks = seq(0, 1194, by = 100), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_color_manual(values = c("#ED1C24","#EF7B10","#0060A8", "#B26300", "#FFD329", "#007D32", "#F4A9BE", "#A1A5A7", "#9B0058", "#0019A8", "#00FFA1", "#9364CC", "cyan", "#6CBE45", "#BD3844")) +
  labs(x = "block", y = "frequency", color="word") +
  ggtitle("frequency of Special Words from C by block of 100 words") +
  theme(axis.title.x = element_text(family = "mono"),
        axis.title.y = element_text(family = "mono"),
        plot.title = element_text(family = "mono"),
        plot.subtitle = element_text(family = "mono"),
        legend.title = element_text(family = "mono"),
        legend.text = element_text(family = "mono"))

plot1
```

Here too, I calculate Pearson correlation coefficients for the above data to check for linear trends:

```{r echo = FALSE, comment = NA}
# run Pearson correlation tests between all specified words and chapters/sections/blocks:

cor_tests_by_chapter <- specified_word_frequency_by_chapter |>
  select(c(2:16)) |>
  apply(2, cor.test, specified_word_frequency_by_chapter$chapter)

cor_tests_by_section <- specified_word_frequency_by_section |>
  select(c(2:16)) |>
  apply(2, cor.test, specified_word_frequency_by_section$section)

cor_tests_by_block <- specified_word_frequency_by_block |>
  select(c(2:16)) |>
  apply(2, cor.test, specified_word_frequency_by_block$block)

# there has to be a better way of getting all the correlation values and p-values to print in a table:

table2 <- matrix(c(cor_tests_by_chapter$c$estimate,
                   cor_tests_by_chapter$c$p.value,
                   cor_tests_by_section$c$estimate,
                   cor_tests_by_section$c$p.value,
                   cor_tests_by_block$c$estimate,
                   cor_tests_by_block$c$p.value,
                   cor_tests_by_chapter$call$estimate,
                   cor_tests_by_chapter$call$p.value,
                   cor_tests_by_section$call$estimate,
                   cor_tests_by_section$call$p.value,
                   cor_tests_by_block$call$estimate,
                   cor_tests_by_block$call$p.value,
                   cor_tests_by_chapter$carbon$estimate,
                   cor_tests_by_chapter$carbon$p.value,
                   cor_tests_by_section$carbon$estimate,
                   cor_tests_by_section$carbon$p.value,
                   cor_tests_by_block$carbon$estimate,
                   cor_tests_by_block$carbon$p.value,
                   cor_tests_by_chapter$centre$estimate,
                   cor_tests_by_chapter$centre$p.value,
                   cor_tests_by_section$centre$estimate,
                   cor_tests_by_section$centre$p.value,
                   cor_tests_by_block$centre$estimate,
                   cor_tests_by_block$centre$p.value,
                   cor_tests_by_chapter$christ$estimate,
                   cor_tests_by_chapter$christ$p.value,
                   cor_tests_by_section$christ$estimate,
                   cor_tests_by_section$christ$p.value,
                   cor_tests_by_block$christ$estimate,
                   cor_tests_by_block$christ$p.value,
                   cor_tests_by_chapter$cipher$estimate,
                   cor_tests_by_chapter$cipher$p.value,
                   cor_tests_by_section$cipher$estimate,
                   cor_tests_by_section$cipher$p.value,
                   cor_tests_by_block$cipher$estimate,
                   cor_tests_by_block$cipher$p.value,
                   cor_tests_by_chapter$cocaine$estimate,
                   cor_tests_by_chapter$cocaine$p.value,
                   cor_tests_by_section$cocaine$estimate,
                   cor_tests_by_section$cocaine$p.value,
                   cor_tests_by_block$cocaine$estimate,
                   cor_tests_by_block$cocaine$p.value,
                   cor_tests_by_chapter$code$estimate,
                   cor_tests_by_chapter$code$p.value,
                   cor_tests_by_section$code$estimate,
                   cor_tests_by_section$code$p.value,
                   cor_tests_by_block$code$estimate,
                   cor_tests_by_block$code$p.value,
                   cor_tests_by_chapter$communication$estimate,
                   cor_tests_by_chapter$communication$p.value,
                   cor_tests_by_section$communication$estimate,
                   cor_tests_by_section$communication$p.value,
                   cor_tests_by_block$communication$estimate,
                   cor_tests_by_block$communication$p.value,
                   cor_tests_by_chapter$connect$estimate,
                   cor_tests_by_chapter$connect$p.value,
                   cor_tests_by_section$connect$estimate,
                   cor_tests_by_section$connect$p.value,
                   cor_tests_by_block$connect$estimate,
                   cor_tests_by_block$connect$p.value,
                   cor_tests_by_chapter$copper$estimate,
                   cor_tests_by_chapter$copper$p.value,
                   cor_tests_by_section$copper$estimate,
                   cor_tests_by_section$copper$p.value,
                   cor_tests_by_block$copper$estimate,
                   cor_tests_by_block$copper$p.value,
                   cor_tests_by_chapter$crypt$estimate,
                   cor_tests_by_chapter$crypt$p.value,
                   cor_tests_by_section$crypt$estimate,
                   cor_tests_by_section$crypt$p.value,
                   cor_tests_by_block$crypt$estimate,
                   cor_tests_by_block$crypt$p.value,
                   cor_tests_by_chapter$cyan$estimate,
                   cor_tests_by_chapter$cyan$p.value,
                   cor_tests_by_section$cyan$estimate,
                   cor_tests_by_section$cyan$p.value,
                   cor_tests_by_block$cyan$estimate,
                   cor_tests_by_block$cyan$p.value,
                   cor_tests_by_chapter$cyst$estimate,
                   cor_tests_by_chapter$cyst$p.value,
                   cor_tests_by_section$cyst$estimate,
                   cor_tests_by_section$cyst$p.value,
                   cor_tests_by_block$cyst$estimate,
                   cor_tests_by_block$cyst$p.value,
                   cor_tests_by_chapter$sea$estimate,
                   cor_tests_by_chapter$sea$p.value,
                   cor_tests_by_section$sea$estimate,
                   cor_tests_by_section$sea$p.value,
                   cor_tests_by_block$sea$estimate,
                   cor_tests_by_block$sea$p.value),
                15, 6, byrow = TRUE)

prmatrix(table2,
         rowlab = c("C", "call", "carbon", "centre", "Christ",
                  "cipher", "cocaine", "code", "communication/Comintern", "connect",
                  "copper", "crypt/cryptography/etc.", "cyan/cyanide", "cyst", "sea"),
         collab = c("chapter (cor)", "(p-value)",
                    "section (cor)", "(p-value)",
                    "block (cor)", "(p-value)"))

```

### Pairwise correlation analysis of specified words:

Finally, I look at which of the 15 words beginning with _C_ specified above are most likely to occur in the same chapter/section of _C_:

```{r}
library(widyr)

word_pairs_by_chapter <- specified_words_filtered_merged |>
  select(chapter, word) |>
  group_by(word) |>
  pairwise_cor(word, chapter, sort = TRUE)

word_pairs_by_section <- specified_words_filtered_merged |>
  select(section, word) |>
  group_by(word) |>
  pairwise_cor(word, section, sort = TRUE)

word_pairs_by_block <- specified_words_filtered_merged |>
  select(block, word) |>
  group_by(word) |>
  pairwise_cor(word, block, sort = TRUE)

top_word_pairs_by_section <- word_pairs_by_section |>
  filter(correlation > .3)
```

Here are the 10 specified words most correlated with "c" by chapter:

```{r}
word_pairs_by_chapter |>
  filter(item1 == "c") |>
  slice_max(correlation, n = 10, with_ties = FALSE)
```

Here are the 10 specified words most correlated with "call" by chapter:

```{r}
word_pairs_by_chapter |>
  filter(item1 == "call") |>
  slice_max(correlation, n = 10, with_ties = FALSE)
```

And here's a visualization of every pair of those specified words that has a greater than 0 correlation within chapters/blocks of _C_:

```{r out.width = "100%", fig.align = "center", dpi = 300}
set.seed(44)

plot1 <- word_pairs_by_chapter |>
  filter(correlation > 0) |>
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "#ED1C24", size = 2.5, alpha = 0.85) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

plot2 <- word_pairs_by_section |>
  filter(correlation > 0) |>
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "#ED1C24", size = 2.5, alpha = 0.85) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

plot3 <- word_pairs_by_block |>
  filter(correlation > 0) |>
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "#ED1C24", size = 2.5, alpha = 0.85) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

plot1 + plot3
```
